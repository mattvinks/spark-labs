<link rel='stylesheet' href='../assets/main.css'/>

[<< back to main index](../README.md)

Lab 9.1 - Cluster install - Generic
====================================

### Overview
Setup and configure Spark as a cluster (standalone)

### Depends On 
None

### Run time
30-60 mins

STEP 1: Select nodes
--------------------
Instructor will provide additional nodes for setup


STEP 2: Shutdown Spark
---------------------
If Spark is running on current node, shut it down
```
$   ~/spark/sbin/stop-all.sh
```


STEP 3: Configure Master Node
-----------------------------
Let's say our Spark cluster is comprised of 3 nodes.
```
master
slave1
slave2
```

### 3.1  ~/spark/conf/slaves
Edit (or create) `~/spark/conf/slaves` file add hostnames - one per line.   
For Amazon cloud be sure to use *internal ip address*
```
master ip address
slave1 ip address
slave2 ip address
```

**Note:** We are also doubling master as a slave in this test cluster.


### 3.2  ~/spark/conf/spark-env.sh
**=> Create `~/spark/conf/spark-env.sh` file**
```
$   cp ~/spark/conf/spark-env.sh.template   ~/spark/conf/spark-env.sh
```

Add the following content to `~/spark/conf/spark-env.sh`
```
#!/usr/bin/env bash
# use external IP
SPARK_MASTER_IP=public_ip_address_of_master_host
# e.g 
#SPARK_MASTER_IP=ec2-54-159-193-182.compute-1.amazonaws.com
```


STEP 4: Distribute Files
------------------------
It is time to distribute Spark files to all nodes in the cluster.  
We will be using  `~/spark-labs/scripts/copy-files.sh`  script to do this.  

### 4.1  `~/hosts` file
Copy   `~/spark/conf/slaves`  files as `~/hosts` file
```
$   cp  ~/spark/conf/slaves     ~/hosts
```


### 4.2  copy files
```
$    ~/spark-labs/scripts/copy-files.sh
```


STEP 5: Start The Cluster
-------------------------
Now that we have the files distributed, ....
```
$   ~/spark/sbin/start-all.sh
```

Note the console output, it will say along the lines of 'starting worker'


STEP 6: Inspect Master UI
-------------------------
Go to `http://master_host:8080` to see the master UI.  
You should see 3 nodes active.

See screen shot below   
<img src="../images/9.1a.png" style="border: 5px solid grey; max-width:100%;"/>


STEP 7: Start Spark Shell 
-------------------------
```
$   ~/spark/bin/spark-shell   --master  spark_master_url

# e.g.
$   ~/spark/bin/spark-shell   --master  spark://ec2-54-159-193-182.compute-1.amazonaws.com:7077
```

See screen shot below   

<img src="../images/9.1b-cluster.png" style="border: 5px solid grey; max-width:100%;"/>

Submit a few applications