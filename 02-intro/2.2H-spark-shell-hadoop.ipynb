{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel='stylesheet' href='../assets/css/main.css'/>\n",
    "\n",
    "# Lab 2.2H: PySpark Shell On Hadoop\n",
    "\n",
    "### Overview\n",
    "Get familiar with Pyspark  \n",
    "\n",
    "### Run time\n",
    "approx. 20-30 minutes\n",
    "\n",
    "### Notes\n",
    "\n",
    "## Step 1 : Login to Hadoop Node\n",
    "Follow instructions for your environment.\n",
    "\n",
    "## Step 2 : Start Spark shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$    pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ==> Configuring UI options **  \n",
    "Spark Shell by default publishes a UI on port number 4040.  \n",
    "How ever when multiple apps are running, and port 4040 already taken, Spark Shell will try to find an open port (4041, 4042 ..etc)\n",
    "\n",
    "Specifying a custom port"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "    $   pyspark  --conf spark.ui.port=4060\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn off UI altogether"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "    $   pyspark  --conf spark.ui.enabled=false\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Set the log level to WARN\n",
    "Type this in Spark Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-457ac98dabfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLogLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WARN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Inspect the Shell UI\n",
    "Look at the console log to identify the Spark Shell UI port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "console"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INFO Utils: Successfully started service 'SparkUI' on port 4042."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In the above example port number of 4042).   \n",
    "\n",
    "Access Spark Shell UI as http://hadoop_node_ip_address:port_number  \n",
    "\n",
    "(Instructor will provide more details)\n",
    "\n",
    "\n",
    "#\n",
    "<img src=\"../assets/images/2a.png\" style=\"border: 5px solid grey ; max-width:100%;\" />\n",
    "\n",
    "**==> Explore stage, storage, environment and executor tabs**\n",
    "\n",
    "**==> Take note of 'Event Timeline', we will use this for monitoring our jobs later**\n",
    "\n",
    "## STEP 5: Spark context\n",
    "Within Spark  shell,  variable `sc` is the SparkContext.  \n",
    "Type `sc` in scala prompt and see what happens.  \n",
    "Your output might look like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see all methods in sc variable, type `sc.` (don't forget the DOT)  and type `TAB`. This will show all the available methods on `sc` variable.\n",
    "\n",
    "Try the following:\n",
    "\n",
    "**==> Print the name of application name**\n",
    "`sc.appName`\n",
    "\n",
    "**==> Find the 'Spark master' for the shell**\n",
    "`sc.master`\n",
    "\n",
    "\n",
    "## Step 6 : Load a local file \n",
    "Let's load  `/etc/hosts` file in Spark Shell.  \n",
    "Issue the following commands in Spark-shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b91453e95103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file:///etc/hosts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "f = sc.textFile(\"file:///etc/hosts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### answer the following questions:\n",
    "\n",
    "**==> What is the 'type' of f ?**   \n",
    "hint : type `f` on the console\n",
    "\n",
    "**==> Inspect Spark Shell UI;  Do you see any processing done?  Why (not)?**\n",
    "\n",
    "**==> Print the first line / record from RDD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==> Again, inspect Spark Shell UI;  do you see any processing done?  Why (not)?**\n",
    "\n",
    "**==> Print first 3 lines of RDD**  \n",
    "hint : `f.take(???)`  (provide the correct argument to take function)\n",
    "\n",
    "**==> Again, inspect Spark Shell UI on port 4040, do you see any processing done?  Why (not)?**\n",
    "\n",
    "**==> Print all the content from the file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==> How many lines are in the file?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala "
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==> Inspect the 'Jobs' section in Shell UI (in browser)**  \n",
    "Also inspect the event time line\n",
    "\n",
    "<img src=\"../assets/images/2b.png\" style=\"border: 5px solid grey; max-width:100%;\" />\n",
    "\n",
    "**==> Inspect the 'Executor' section in Shell UI (in browser)**\n",
    "\n",
    "<img src=\"../assets/images/2c.png\" style=\"border: 5px solid grey; max-width:100%;\" />\n",
    "\n",
    "\n",
    "## Step 7 : Load a HDFS file\n",
    "Let's load  a sample data from `/data/twinkle/'.  \n",
    "Try the following in Spark-shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e57442ca2c53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/text/twinkle/sample.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#count the lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "a = sc.textFile(\"data/text/twinkle/sample.txt\")\n",
    "\n",
    "#count the lines\n",
    "a.count()\n",
    "\n",
    "#print the lines\n",
    "a.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 : Use Spark Session (Only in V2 and later)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get `SparkContext` from `SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==> Quit spark-shell session `Control + D`**\n",
    "\n",
    "\n",
    "\n",
    "## STEP 9:  Connecting Spark Shell to YARN\n",
    "\n",
    "**==> Quit spark-shell session, if you haven't done so yet.. `Control + D`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "```bash\n",
    "    $ pyspark  --master yarn  --name 'MY_NAME spark shell'\n",
    "    # couldnt connect to yarn cluster.\n",
    "    # instead of yarn using local master to connect spark://ip-172-31-47-253.us-west-2.compute.internal:7077\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the shell starts, check the 'Resource Manager UI' on your Hadoop system.  Do you see the spark shell connected?\n",
    "\n",
    "Try the following while the Spark shell is connected to YARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = sc.textFile(\"data/text/twinkle/sample.txt\")\n",
    "f.count()\n",
    "f.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tip : Dealing With Logs\n",
    "Spark Shell by default prints logs at warning (WARN) level.  If you want to change the logging level, do this at Spark shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't want to see any logs, you can start Spark shell as follows.  All the logs will be sent to 'logs' file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pyspark  2> logs\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
