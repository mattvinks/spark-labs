<link rel='stylesheet' href='../assets/css/main.css'/>

[<< back to main index](../README.md)

Lab 4.6 : Data formats (JSON vs. Parquet vs. ORC)
==========================================

### Overview
Comparing different data formats for Dataframes.  We will evaluate JSON, Parquet and ORC format.

Background reads:
- [Spark data frames](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- JSON format
    - [wikipedia](https://en.wikipedia.org/wiki/JSON)
    - [json.org](http://json.org/)
- Parquet format
    - [Parquet project](https://parquet.apache.org/)
    - [parquet github](https://github.com/Parquet/parquet-format)
    - [presentation](http://www.slideshare.net/larsgeorge/parquet-data-io-philadelphia-2013)
- ORC format
    + [ORC project](https://orc.apache.org/)
    + [ORC explained](http://www.semantikoz.com/blog/orc-intelligent-big-data-file-format-hadoop-hive/)
    + [ORC performance](http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.3/bk_performance_tuning/content/hive_perf_best_pract_use_orc_file_format.html)

### Depends On
None

### Run time
20-30 mins


### STEP 1: Clickstream data
There is about 1G+ clickstream data stored in `/data/click-stream/json` directory.  

**If you don't have this data**, you can generate it as follows
```bash
    $   cd  /data/click-stream/
    $   python gen-clickstream-json.py
```

**=> Inspect json logs**  

```bash
    $    head /data/click-stream/json/2015-01-01.json
```


### STEP 2: Benchmarking Spreadsheet
Download and inspect [Benchmarking_Dataformats.xlsx](Benchmarking_Dataformats.xlsx).  
**We will be filling out the values in this spreadsheet, as we execute commands on Spark Shell.**

It will look like this (click on the image for larger version)

<a href="../assets/images/5.3a.png"><img src="../assets/images/5.3a-small.png" style="border: 5px solid grey; max-width:100%;"/></a>


### STEP 3: Start Spark Shell & ATOP


```bash
    # start spark shell with more memory
    $    ~/spark/bin/spark-shell --executor-memory 2g  --driver-memory 1g
```


Set the log level to INFO (so we can measure time taken for each job)
```scala
    scala>
            sc.setLogLevel("INFO")
```

Also open another terminal and run `atop`.  
We will use this to monitor CPU / IO usage

### STEP 4: Setup SQL Imports

**This is not necessary in Spark Shell, but needed in Spark applications**
```scala
    // this is used to implicitly convert an RDD to a DataFrame.
    import spark.implicits._
```

### STEP 5: Load Clickstream data
```scala
    // load all the files in the dir
    val clicksJson = spark.read.json("/data/click-stream/json/")
```

**==> While the import is running take a look at `atop` terminal.  Which of the resources are we maxing out?**  
**==> Measure the time taken to load JSON data; record it in the spreadsheet**  

**==> Find the max value of cost**   
**==> While the query is running, check `atop`**  

```scala
    val maxCost = clicksJson.agg(max("cost"))
    maxCost.show
```

Sample output
```console
    +---------+
    |MAX(cost)|
    +---------+
    |      180|
    +---------+
```

**==> Note the time it took to run the query, and record it in spreadsheet**

```console
    Job 1 finished: show at <console>:24, took `8.550481 s`
```


### STEP 6 : Save the logs in Parquet format

We are going to use Spark's built-in parquet support to save the dataframe into parquet format

```scala
    clicksJson.write.parquet("/data/click-stream/my-parquet")
```

**==> Inspect `atop`**  
**==> Measure the time taken to 'save as parquet' and record it in spreadsheet**  
**==> Once the job is completed, inspect the `/data/click-stream/my-parquet` directory**

```bash
    $   ls -la   /data/click-stream/my-parquet
```


Output might look like this

```console
    -rw-r--r--  1 sujee  staff     0B Jul 19 12:40 _SUCCESS
    -rw-r--r--  1 sujee  staff   1.8M Jul 19 12:39 part-r-00000-9ceb13fe-a57c-4af1-993e-d998d6c41008.gz.parquet
    -rw-r--r--  1 sujee  staff   1.8M Jul 19 12:39 part-r-00001-9ceb13fe-a57c-4af1-993e-d998d6c41008.gz.parquet
```


## Step 7 : Saving ORC
```scala
    clicksJson.write.orc("/data/click-stream/my-orc")
```

**==> Measure the time taken to save as ORC and record in spreadsheet**   

## STEP 8 : Querying Parquet Data

```scala
    val clicksParquet = spark.read.parquet("/data/click-stream/my-parquet")
```

**==> Note how quickly the data is loaded; measure this time and record in spreadsheet**   
**==> and schema is inferred!**  

Parquet format has built-in schema, so Spark doesn't have to parse the files as needed in JSON format

**==> Caclculate max(cost)**   

```scala
    clicksParquet.agg(max("cost")).show  // same as before
```

**==> Notice the time took and record in spreadsheet**    
Sample output

```console
    Job 3 finished: show at <console>:24, took `0.627185 s`
```


**==> Why parquet is so quick to process?**


## STEP 9 : Querying ORC
```scala
    val clicksORC = spark.read.orc("/data/click-stream/my-orc")
```

**==> Note the load time and record in spreadsheet**   

**==> Measure query time and record in spreadsheet**  
```scala
    clicksORC.agg(max("cost")).show  // same as before
```


## Step 10 : Compare Data Sizes
Enter the size in bytes in spreadsheet**  
```bash
    # bytes for spreadsheet
    $    du -b  /data/click-stream/*
    # in Mac use `du -k`

    # for human readable format use
    $    du -skh  /data/click-stream/*
    # or
    $    du -h  /data/click-stream/*
```


Sample output

```console
    1415178847  /Users/sujee/data/click-stream/json
    161398938   /Users/sujee/data/click-stream/json-gz
    105793926   /Users/sujee/data/click-stream/my-orc
    118394196   /Users/sujee/data/click-stream/my-parquet
```

**==> Record the byte sizes in spreadsheet**   

### STEP 8 : Compressed JSON

We are going to store JSON files in compressed gzip format

**==> Compress the files**  

    $    cd   /data/click-stream
    $   ./compress-json.sh

This will create compressed JSON in `json-gz` directory

**==> Inspect directory sizes**  

```
    # bytes for spreadsheet
    $    du -b json    json-gz   my-parquet  my-orc

    # human readable format
    $    du -skh  json    json-gz   my-parquet   my-orc
```

Sample output

    1.3G    json
    154M    json-gz
     77M    my-parquet


**==> Load compressed json files in Spark shell and do the same processing**  
**==> Look at `atop` window to see resource usage**  

    // note the parsing time
    val clicksJgz = spark.read.json("/data/click-stream/json-gz")

    // calculate the max cost
    // notice the time took
    clicksJgz.agg(max("cost")).show

    // output : Job 7 finished: show at console:22, took 8.066727 s


### STEP 9 : Analyze / discuss results


    Here are numbers from my run:

    |format   | storage size |  loading time | query time : max(cost)|
    |---------|:-------------|:--------------|:---------------------:|
    | json    |  1.3 G       |  8.3 s        |   4.6 s               |
    | json.gz |  154 M       |  8.5 s        |   4.1 s               |
    | parquet |  101 M       |    0 s        |   0.23 s              |
    | ORC     |  113 M       |    0 s        |   0.76 s              |


**==> Also discuss your findings from `atop`.  Which resource 'ceiling' we are hitting first?  CPU / Memory / Disk ?**
