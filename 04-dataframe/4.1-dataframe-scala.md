<link rel='stylesheet' href='../assets/css/main.css'/>

[<< back to main index](../README.md)

Lab 4.1 : Spark SQL : Dataframes
================================

### Overview
First look at Spark SQL

### Depends On
None

### Run time
20-30 mins

## Step 1 : Inspect Clickstream data

The data is in [/data/click-stream/clickstream.json](/data/click-stream/clickstream.json)

Clickstream data looks like this
```
{"session": "session_36", "domain": "youtube.com", "cost": 118, "user": "user_9", "campaign": "campaign_19", "ip": "ip_4", "action": "clicked", "timestamp": 1420070400000}

{"session": "session_96", "domain": "facebook.com", "cost": 5, "user": "user_5", "campaign": "campaign_12", "ip": "ip_3", "action": "blocked", "timestamp": 1420070400864}
```



## STEP 2: Start Spark Shell

Launch Shell:
```bash
    $   ~/spark/bin/spark-shell
```


## STEP 3: Load Clickstream data

```scala

    // This import is needed to use the $-notation
    import spark.implicits._

    val clickstreamDF = spark.read.json("/data/click-stream/clickstream.json")
```    

**==> Spark will process the file to infer the schema ;  See console output**  

```console
    clickstreamDF: org.apache.spark.sql.DataFrame = [action: string, campaign: string,
    cost: bigint, domain: string, ip: string, session: string,
    timestamp: bigint, user: string]
```

**==> Monitor Spark shell UI on port 4040**  
You may see something like this:

<img src="../assets/images/5.1a.png" style="border: 5px solid grey; max-width:100%;" />

**==> Q : Why is Spark not lazy loading the JASON files?**

## STEP 4 : Inspecting The Dataframe

**==> Print the schema of data frame**     
Hint : `clickstreamDF.printSchema`  
Your output will look like this:

``` console

    root
     |-- action: string (nullable = true)
     |-- campaign: string (nullable = true)
     |-- cost: long (nullable = true)
     |-- domain: string (nullable = true)
     |-- ip: string (nullable = true)
     |-- session: string (nullable = true)
     |-- timestamp: long (nullable = true)
     |-- user: string (nullable = true)
```



**==> Print / Dump the data contained within dataframe**  
Hint : `clickstreamDF.show`

Your output may look like this:  

```console
+-------+-----------+----+-----------------+----+----------+-------------+------+
| action|   campaign|cost|           domain|  ip|   session|    timestamp|  user|
+-------+-----------+----+-----------------+----+----------+-------------+------+
|clicked|campaign_19| 118|      youtube.com|ip_4|session_36|1420070400000|user_9|
|blocked|campaign_12|   5|     facebook.com|ip_3|session_96|1420070400864|user_5|
|clicked| campaign_3|  54|sf.craigslist.org|ip_9|session_61|1420070401728|user_8|
...


```

**==> Explore methods available in Dataframe**  
1) Here is the Dataframe API :
[Scala](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame)  /
[Java](http://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrame.html) /
[Python](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dataframe#pyspark.sql.DataFrame)

2) Type `clickstreamDF. TAB ` (hit the TAB key)  to see all methods available on DataFrame.  

```console

    scala> clickstreamDF.
    agg                 apply               as                  asInstanceOf        cache
    col                 collect             collectAsList       columns             count
    createJDBCTable     describe            distinct            dtypes              except
    explain             explode             filter              first               flatMap
    foreach             foreachPartition    groupBy             head                insertInto
    insertIntoJDBC      intersect           isInstanceOf        isLocal             javaRDD
    join                limit               map                 mapPartitions       na
    orderBy             persist             printSchema         queryExecution      rdd
    registerTempTable   repartition         sample              save                saveAsParquetFile
    saveAsTable         schema              select              selectExpr          show
    sort                sqlContext          take                toDF                toJSON
    toJavaRDD           toSchemaRDD         toString            unionAll            unpersist
    where               withColumn          withColumnRenamed

```

## STEP 5 : Querying Dataframe

**==> Show only click logs where the cost > 100**  
```scala
    clickstreamDF.filter(clickstreamDF("cost") > 100).show()
    clickstreamDF.filter("cost > 100").show()
    clickstreamDF.filter($"cost" > 100).show() // need :  import spark.implicits._
```

Sample output

```console

    +-------+-----------+----+-----------------+----+----------+-------------+------+
    | action|   campaign|cost|           domain|  ip|   session|    timestamp|  user|
    +-------+-----------+----+-----------------+----+----------+-------------+------+
    |clicked|campaign_19| 118|      youtube.com|ip_4|session_36|1420070400000|user_9|
    |blocked|campaign_18| 110|    wikipedia.org|ip_5|session_55|1420070402592|user_6|
    |blocked| campaign_9| 139|          cnn.com|ip_8|session_13|1420070404320|user_7|
```

**==> Show the logs where action = clicked**  
Hint : `clickstreamDF.filter("action == '???'")`  

## Step 6 : Explore methods in Column type

```
    val c = clickstreamDF("action")
    // c: org.apache.spark.sql.Column = action

    // to see methods on c use TAB
    c.[TAB]
```

You will see methods avilable on on `org.apache.spark.sql.Column`

```console

scala>
    c.
    !==            %              &&             *              +              -
    /              ===            >              >=             and            as
    asInstanceOf   asc            cast           contains       desc           divide
    endsWith       eqNullSafe     equalTo        explain        geq            getField
    getItem        gt             in             isInstanceOf   isNotNull      isNull
    leq            like           lt             minus          mod            multiply
    notEqual       or             plus           rlike          startsWith     substr
    toString       unary_!        unary_-        ||
```

Use `===`  or `equalTo`

```scala

    clickstreamDF.filter(clickstreamDF("action") === "clicked" ).show
    // or
    clickstreamDF.filter(clickstreamDF("action").equalTo("clicked")).show
```

## Step 7: Find all unique domains
Hint : `distinct` on the right column name
```scala
  clickstreamDF.select("??? name of column ???").distinct
```

## Step 8 : Count the number of visits from each domain

Hint : `val g = clickstreamDF.groupBy("column_name")`  
* Inspect methods on `g`  (use tab completion)  
* Then do `count` on `g`  
* Then do a show  
* So `clickstreamDF.groupBy().count.show`  


## STEP 9 : Joining Dataframes

Let's load another data set `domain info`  
The data is in   `data/click-stream/domain-info.json`  
The data looks like this:

```console

    {"domain":"amazon.com","category":"SHOPPING"}
    {"domain":"bbc.co.uk","category":"NEWS"}
    {"domain":"facebook.com","category":"SOCIAL"}
    ...
```

**==> Load the dataframe**

```scala

    val domainsDF = spark.read.json("/data/click-stream/domain-info.json")
```

**==> Join both dataframes**

```scala

    val joined = clickstreamDF.join(domainsDF,  clickstreamDF("domain") === domainsDF("domain"))

    // see the results
    joined.show
```

**==> Inspect the results, here is a sample**

```console

    +-------+-----------+----+-----------------+----+----------+-------------+------+-----------+-----------------+
    | action|   campaign|cost|           domain|  ip|   session|    timestamp|  user|   category|           domain|
    +-------+-----------+----+-----------------+----+----------+-------------+------+-----------+-----------------+
    |clicked|campaign_19| 118|      youtube.com|ip_4|session_36|1420070400000|user_9|      VIDEO|      youtube.com|
    |blocked| campaign_2|   7|      youtube.com|ip_2|session_93|1420070412960|user_1|      VIDEO|      youtube.com|
    |blocked|campaign_17|  20|       amazon.com|ip_4|session_13|1420070406048|user_1|   SHOPPING|       amazon.com|
```

**==> Note some rows are missing.  Which ones?  Why?**

**==> Do an outer join**    
Hint : provide a third argument `outer` to the join statement  
e.g  `val joinedOuter = clickstreamDF.join(domainsDF,  ......,    "outer")`

**==> Inspect the output, might look like this**  

**==> Can you explain the null values?**  

```console

    // joinedOuter.show

    // result ==>

    +-------+-----------+----+-----------------+----+----------+-------------+------+-----------+-----------------+
    | action|   campaign|cost|           domain|  ip|   session|    timestamp|  user|   category|           domain|
    +-------+-----------+----+-----------------+----+----------+-------------+------+-----------+-----------------+
    |blocked| campaign_9| 139|          cnn.com|ip_8|session_13|1420070404320|user_7|       null|             null|
    |   null|       null|null|             null|null|      null|         null|  null|     SOCIAL|      twitter.com|
    |clicked| campaign_6|  15|comedycentral.com|ip_9|session_49|1420070403456|user_4|       null|             null|

```


## Step 10 : Understand Query Execution
We will use `explain` keyword to see how Spark is optimizing and executing the query.

```scala

clickstreamDF.filter("cost > 100").explain(extended=true)

joined.explain(extended=true)

joinedOuter.explain(extended=true)

```
