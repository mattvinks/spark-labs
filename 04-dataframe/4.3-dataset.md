<link rel='stylesheet' href='../assets/css/main.css'/>

[<< back to main index](../README.md)

Lab 4.3 : Dataset
=================


### Overview
Get comfortable with Spark Dataset (and RDD / Dataframe)

### Depends On
None

### Run time
20-30 mins


## Step 1: Start Spark Shell
Change working directory to `spark-labs`.  This way, we can access data using relative paths (makes life simpler)

```bash
    $  cd ~/spark-labs
    $   ~/spark/bin/spark-shell
```

## Step 2: Imports

```scala
// This is used to implicitly convert an RDD to a DataFrame.
import spark.implicits._
import org.apache.spark.sql._
import org.apache.spark.sql.types._
```

## Step 3: Use CSV Reader

Here is  [data/people/pepole2.csv](../data/people/people2.csv)
```
name,gender,age
John,M,35
Jane,F,40
Mike,M,18
Sue,F,19
```

Load it using CSV loader

```scala
val peopleDF = spark.read.
               option("header", "true").
               csv("data/people/people2.csv")
peopleDF.columns
peopleDF.printSchema
```

**==> What is the type for `peopleDF` ?**


## Step 4:  Supplying schema using `StructField`

```scala

// == read DF using csv reader with schema
// TODO : correctly assign the types : StringType / IntegerType
val nameField = StructField("name", StringType)
val genderField = StructField("gender", ???)
val ageField = StructField("age", ???)
val peopleSchema = StructType(Array(nameField, genderField, ageField))

val peopleDF2 = spark.read.
                option("header", "true").
                schema(peopleSchema).
                csv("data/people/people2.csv")

peopleDF2.printSchema
```


## Step 5: Reading RDD with schema

```scala
// TODO : assign types  (String / Integer)
final case class Person (
    name: ???,
    gender: ???,
    age:???
  )

```

Read RDD as text first

```scala
val p = spark.sparkContext.textFile("data/people/people.csv")
```

Then parse it into `Person` class
```scala
val peopleRDD = p.map (line => {
        val tokens = line.split(",")
        val name = tokens(0)
        val gender = tokens(1)
        val age = tokens(2).toInt
        // TODO : fill in Person class
        new Person (???, ???, ???)
      })
peopleRDD.collect.foreach(println)
```

## Step 6: Conversion across RDD / Dataframe / Dataset

**==> Convert RDD to Dataset**

```scala
// TODO use `toDS` function
// Hint : see slides for code snippets
val peopleDS = peopleRDD.???
peopleDS.show

// another approach
// TODO : supply the class name and RDD
val peopleDS2 = spark.createDataset[?? class name ??](?? RDD ??)
peopleDS2.show
```

**==> Access underlying RDD from `peopleDS` **
```scala
peopleDS.???
```

## Step 7: Type safe operations on Dataset

**==> Find all people with age > 20  **

```scala
// 'age' is treated like Integer
peopleDS.filter(_.age > ???).show
```

**==> Print names in UPPERCASE**
```scala
// Hint : search for how to convert Scala string to uppercase
peopleDS.map( p => p.name.???).show
```

## Step 8 : Dataset -> Dataframe -> RDD

**==> Convert Dataset to Dataframe**
```scala
val df2 = peopleDS.toDF
```

**==> convert Dataframe to Dataset**
```scala
val ds2 = df2.as[?? Class Name ??]
```

**==> Access underlying RDD within Dataframe**
```scala
df2.rdd
```
