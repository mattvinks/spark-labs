{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel='stylesheet' href='../assets/css/main.css'/>\n",
    "\n",
    "[<< back to main index](../README.md) \n",
    "\n",
    "# Lab 4.6 : Data formats (JSON vs. Parquet vs. ORC)\n",
    "\n",
    "\n",
    "### Overview\n",
    "Comparing different data formats for Dataframes.  We will evaluate JSON, Parquet and ORC format.\n",
    "\n",
    "Background reads:\n",
    "- [Spark data frames](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- JSON format \n",
    "    - [wikipedia](https://en.wikipedia.org/wiki/JSON)\n",
    "    - [json.org](http://json.org/)\n",
    "- Parquet format\n",
    "    - [Parquet project](https://parquet.apache.org/)\n",
    "    - [parquet github](https://github.com/Parquet/parquet-format)\n",
    "    - [presentation](http://www.slideshare.net/larsgeorge/parquet-data-io-philadelphia-2013)\n",
    "- ORC format\n",
    "    + [ORC project](https://orc.apache.org/)\n",
    "    + [ORC explained](http://www.semantikoz.com/blog/orc-intelligent-big-data-file-format-hadoop-hive/)\n",
    "    + [ORC performance](http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.3/bk_performance_tuning/content/hive_perf_best_pract_use_orc_file_format.html)\n",
    "\n",
    "### Depends On \n",
    "None\n",
    "\n",
    "### Run time\n",
    "20-30 mins\n",
    "\n",
    "\n",
    "## STEP 1: Clickstream data\n",
    "There is about 1G+ clickstream data stored in `/data/click-stream/json` directory.  \n",
    "\n",
    "They look like this\n",
    "\n",
    "```json\n",
    "{\"timestamp\": 1420070400000, \"ip\": \"ip_557\", \"user\": \"user_13011\", \"action\": \"blocked\", \"domain\": \"npr.org\", \"campaign\": \"campaign_13\", \"cost\": 116, \"session\": \"session_43\"}\n",
    "\n",
    "{\"timestamp\": 1420070400043, \"ip\": \"ip_129\", \"user\": \"user_58773\", \"action\": \"clicked\", \"domain\": \"flickr.com\", \"campaign\": \"campaign_7\", \"cost\": 170, \"session\": \"session_23\"}\n",
    "\n",
    "{\"timestamp\": 1420070400086, \"ip\": \"ip_704\", \"user\": \"user_71191\", \"action\": \"viewed\", \"domain\": \"foxnews.com\", \"campaign\": \"campaign_20\", \"cost\": 47, \"session\": \"session_48\"}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] If you need to generate more data....\n",
    "```bash\n",
    "    $    cd   /data/click-stream/\n",
    "    $    python   gen-clickstream-json.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Benchmarking Spreadsheet\n",
    "Download and inspect [Benchmarking_Dataformats.xlsx](Benchmarking_Dataformats.xlsx).  \n",
    "**We will be filling out the values in this spreadsheet, as we execute commands on Spark Shell.**\n",
    "\n",
    "It will look like this (click on the image for larger version)\n",
    "\n",
    "<a href=\"../assets/images/5.3a.png\"><img src=\"../assets/images/5.3a-small.png\" style=\"border: 5px solid grey; max-width:100%;\"/></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Spark...\n",
      "Spark found in :  /home/ubuntu/spark\n",
      "Spark config:\n",
      "\t spark.app.name=TestApp\n",
      "\tspark.master=local[*]\n",
      "\texecutor.memory=2g\n",
      "\tspark.sql.warehouse.dir=/tmp/tmp5_vdi3ry\n",
      "\tsome_property=some_value\n",
      "Spark UI running on port 4043\n"
     ]
    }
   ],
   "source": [
    "# initialize Spark Session\n",
    "import os\n",
    "import sys\n",
    "top_dir = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "if top_dir not in sys.path:\n",
    "    sys.path.append(top_dir)\n",
    "\n",
    "from init_spark import init_spark\n",
    "spark = init_spark()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: ATOP\n",
    "\n",
    "Also open another terminal and run **atop**.  \n",
    "If you are using Jupyter Labs, launch a terminal from 'Launcher'.  \n",
    "We will use this to monitor CPU / IO usage \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: Load Clickstream data\n",
    "**==> While the import is running take a look at `atop` terminal.  Which of the resources are we maxing out?**  \n",
    "**==> Measure the time taken to load JSON data; record it in the spreadsheet**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read JSON in 14,570.65 ms \n",
      "DataFrame[action: string, campaign: string, cost: bigint, domain: string, ip: string, session: string, timestamp: bigint, user: string]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# load all the files in the dir\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "clicksJson = spark.read.json(\"/data/click-stream/json/\")\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print (\"Read JSON in {:,.2f} ms \".format( (t2-t1)*1000))\n",
    "print(clicksJson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 - Query \n",
    "**==> Find the max value of cost**   \n",
    "**==> While the query is running, check `atop`**\n",
    "\n",
    "\n",
    "Sample output\n",
    "```\n",
    "    +---------+\n",
    "    |MAX(cost)|\n",
    "    +---------+\n",
    "    |      180|\n",
    "    +---------+\n",
    "```\n",
    "\n",
    "**==> Note the time it took to run the query, and record it in spreadsheet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|max(cost)|\n",
      "+---------+\n",
      "|      180|\n",
      "+---------+\n",
      "\n",
      "MAX in JSON in 6,059.38 ms \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import *\n",
    "\n",
    "clicksJson.createOrReplaceTempView(\"clicks_json\")\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "spark.sql(\"SELECT MAX(cost) FROM clicks_json\").show()\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print (\"MAX in JSON in {:,.2f} ms \".format( (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5 : Save the logs in Parquet format\n",
    "\n",
    "We are going to use Spark's built-in parquet support to save the dataframe into parquet format\n",
    "\n",
    "**==> Inspect `atop` terminal**  \n",
    "**==> Measure the time taken to 'save as parquet' and record it in spreadsheet**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote Parquet in 21,754.87 ms \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "clicksJson.write.parquet(\"/data/click-stream/my-parquet\")\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print (\"Wrote Parquet in {:,.2f} ms \".format( (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Saving ORC\n",
    "\n",
    "**==> Measure the time taken to save as ORC and record in spreadsheet**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote ORC in 23,731.34 ms \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "clicksJson.write.orc(\"/data/click-stream/my-orc\")\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print (\"Wrote ORC in {:,.2f} ms \".format( (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7 : Querying Parquet Data\n",
    "\n",
    "**==> Note how quickly the data is loaded; measure this time and record in spreadsheet**   \n",
    "**==> and schema is inferred!**  \n",
    "\n",
    "Parquet format has built-in schema, so Spark doesn't have to parse the files as needed in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read Parquet in 141.38 ms \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "clicksParquet = spark.read.parquet(\"/data/click-stream/my-parquet\")\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print (\"Read Parquet in {:,.2f} ms \".format( (t2-t1)*1000))\n",
    "\n",
    "clicksParquet.createOrReplaceTempView(\"clicks_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5  - Query Parquet\n",
    "\n",
    "**==> Notice the time took and record in spreadsheet**    \n",
    "**==> Why parquet is so quick to process?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|max(cost)|\n",
      "+---------+\n",
      "|      180|\n",
      "+---------+\n",
      "\n",
      "MAX Parquet in 447.72 ms \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "spark.sql(\"SELECT MAX(cost) FROM clicks_parquet\").show()\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print (\"MAX Parquet in {:,.2f} ms \".format( (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8 : Load ORC\n",
    "**==> Note the load time and record in spreadsheet**   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [
      "scala "
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read ORC in 42.09 ms \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "clicksORC = spark.read.orc(\"/data/click-stream/my-orc\")\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print (\"Read ORC in {:,.2f} ms \".format( (t2-t1)*1000))\n",
    "\n",
    "clicksORC.createOrReplaceTempView(\"clicks_orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 - Query ORC\n",
    "\n",
    "**==> Measure query time and record in spreadsheet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|max(cost)|\n",
      "+---------+\n",
      "|      180|\n",
      "+---------+\n",
      "\n",
      "MAX ORC in 708.59 ms \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "spark.sql(\"SELECT MAX(cost) FROM clicks_orc\").show()\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print (\"MAX ORC in {:,.2f} ms \".format( (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "console"
     ],
     "id": ""
    }
   },
   "source": [
    "## Step 9 : Compare Data Sizes\n",
    "\n",
    "sample output \n",
    "```\n",
    "1.3G\t/data/click-stream/json\n",
    "118M\t/data/click-stream/my-parquet\n",
    "101M\t/data/click-stream/my-orc\n",
    "```\n",
    "\n",
    "**==> Record the byte sizes in spreadsheet**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4G\t/data/click-stream/json\n",
      "113M\t/data/click-stream/my-parquet\n",
      "100M\t/data/click-stream/my-orc\n"
     ]
    }
   ],
   "source": [
    "# for human readable format\n",
    "!  du -skh  /data/click-stream/json  /data/click-stream/my-parquet  /data/click-stream/my-orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1415191006\t/data/click-stream/json\n",
      "118405632\t/data/click-stream/my-parquet\n",
      "104521112\t/data/click-stream/my-orc\n"
     ]
    }
   ],
   "source": [
    "# size in bytes for spreadsheet\n",
    "! du -b /data/click-stream/json  /data/click-stream/my-parquet  /data/click-stream/my-orc\n",
    "\n",
    "# in Mac use `du -k`\n",
    "# ! du -k /data/click-stream/json  /data/click-stream/my-parquet  /data/click-stream/my-orc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS : Compressed JSON\n",
    "\n",
    "We are going to store JSON files in compressed gzip format\n",
    "\n",
    "**==> Compress the files**\n",
    "\n",
    "```bash\n",
    "$    cd   ~/data/click-stream\n",
    "$   ./compress-json.sh\n",
    "```\n",
    "\n",
    "This will create compressed JSON in `json-gz` directory\n",
    "\n",
    "**==> Inspect directory sizes**\n",
    "\n",
    "```bash\n",
    "    # bytes for spreadsheet\n",
    "    $    du -b json    json-gz   parquet \n",
    "\n",
    "    # human readable format\n",
    "    $    du -skh  json    json-gz   parquet \n",
    "```\n",
    "\n",
    "Sample output\n",
    "\n",
    "```\n",
    "1.3G    json\n",
    "154M    json-gz\n",
    " 77M    parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==> Load compressed json files in Spark shell and do the same processing**  \n",
    "**==> Look at `atop` window to see resource usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|max(cost)|\n",
      "+---------+\n",
      "|      180|\n",
      "+---------+\n",
      "\n",
      "Json-gz quiery in 156.55 ms \n"
     ]
    }
   ],
   "source": [
    "#note the parsing time\n",
    "import time\n",
    "\n",
    "\n",
    "clicksJgz = spark.read.json(\"/data/click-stream/json-gz\")\n",
    "\n",
    "\n",
    "clicksParquet.createOrReplaceTempView(\"clicks_jsongz\")\n",
    "\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "# calculate the max cost\n",
    "#notice the time took\n",
    "spark.sql(\"SELECT MAX(cost) FROM clicks_jsongz\").show()\n",
    "t2 = time.perf_counter()\n",
    "print (\"Json-gz quiery in {:,.2f} ms \".format( (t2-t1)*1000))\n",
    "# output : Job 7 finished: show at console:22, took 8.066727 s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 9 : Analyze / discuss results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are numbers from my run:\n",
    "\n",
    "```\n",
    "|format   | storage size |  loading time | query time : max(cost)|\n",
    "|---------|:-------------|:--------------|:---------------------:|\n",
    "| json    |  1.3 G       |  8.3 s        |   4.6 s               |\n",
    "| json.gz |  154 M       |  8.5 s        |   4.1 s               | \n",
    "| parquet |  101 M       |    0 s        |   0.23 s              | \n",
    "| ORC     |  113 M       |    0 s        |   0.76 s              | \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==> Also discuss your findings from `atop`.  Which resource 'ceiling' we are hitting first?  CPU / Memory / Disk ?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
