<link rel='stylesheet' href='../assets/main.css'/>

[<< back to main index](../README.md) 

Lab 5.1 : Spark SQL : Dataframes
================================

### Overview
First look at Spark SQL

### Depends On 
None

### Run time
20-30 mins


----------------------------
STEP 1: Start Spark Shell
----------------------------
Change working directory to `spark-labs`.  This way, we can access data using relative paths (makes life simpler)

    $  cd ~/spark-labs

Launch Shell:

    $   ~/spark/bin/spark-shell


----------------------------
STEP 2: Load Clickstream data
----------------------------

```scala
   // scala
    
    val clickstreamDF = sqlContext.read.json("data/click-stream/clickstream.json")
```    

**==> Spark will process the file to infer the schema ;  See console output**  

```console
    clickstreamDF: org.apache.spark.sql.DataFrame = [action: string, campaign: string, 
    cost: bigint, domain: string, ip: string, session: string, 
    timestamp: bigint, user: string]
```

**==> Monitor Spark shell UI on port 4040**  
You may see something like this:

<img src="../images/6.1a.png" style="border: 5px solid grey; max-width:100%;" />

**==> Q : Why is Spark not lazy loading the JASON files? **

----------------------------
STEP 3 : Inspecting The Dataframe
----------------------------

**==> Print the schema of data frame**     
Hint : `clickstreamDF.printSchema`  
Your output will look like this:

``` console

    root
     |-- action: string (nullable = true)
     |-- campaign: string (nullable = true)
     |-- cost: long (nullable = true)
     |-- domain: string (nullable = true)
     |-- ip: string (nullable = true)
     |-- session: string (nullable = true)
     |-- timestamp: long (nullable = true)
     |-- user: string (nullable = true)
```



**==> Print / Dump the data contained within dataframe**  
Hint : `clickstreamDF.show`

Your output may look like this:  

```console

    +-------+-----------+----+-----------------+----+----------+-------------+------+
    | action|   campaign|cost|           domain|  ip|   session|    timestamp|  user|
    +-------+-----------+----+-----------------+----+----------+-------------+------+
    |clicked|campaign_19| 118|      youtube.com|ip_4|session_36|1420070400000|user_9|
    |blocked|campaign_12|   5|     facebook.com|ip_3|session_96|1420070400864|user_5|
    |clicked| campaign_3|  54|sf.craigslist.org|ip_9|session_61|1420070401728|user_8|
    ...
```

**==> Explore methods available in Dataframe**  
1) Here is the Dataframe API : 
[Scala](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame)  /
[Java](http://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrame.html) / 
[Python](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dataframe#pyspark.sql.DataFrame)

2) Type `clickstreamDF. TAB TAB` (two tabs)  to see all methods available on DataFrame.  

```console

    scala> clickstreamDF.
    agg                 apply               as                  asInstanceOf        cache
    col                 collect             collectAsList       columns             count
    createJDBCTable     describe            distinct            dtypes              except
    explain             explode             filter              first               flatMap
    foreach             foreachPartition    groupBy             head                insertInto
    insertIntoJDBC      intersect           isInstanceOf        isLocal             javaRDD
    join                limit               map                 mapPartitions       na
    orderBy             persist             printSchema         queryExecution      rdd
    registerTempTable   repartition         sample              save                saveAsParquetFile
    saveAsTable         schema              select              selectExpr          show
    sort                sqlContext          take                toDF                toJSON
    toJavaRDD           toSchemaRDD         toString            unionAll            unpersist
    where               withColumn          withColumnRenamed

```

---------------------------------
STEP 4 : Querying Dataframe
---------------------------------

**==> Show only click logs where the cost > 100**  
Hint : `clickstreamDF.filter(clickstreamDF("cost") > 100).show()`

Sample output

```console

    +-------+-----------+----+-----------------+----+----------+-------------+------+
    | action|   campaign|cost|           domain|  ip|   session|    timestamp|  user|
    +-------+-----------+----+-----------------+----+----------+-------------+------+
    |clicked|campaign_19| 118|      youtube.com|ip_4|session_36|1420070400000|user_9|
    |blocked|campaign_18| 110|    wikipedia.org|ip_5|session_55|1420070402592|user_6|
    |blocked| campaign_9| 139|          cnn.com|ip_8|session_13|1420070404320|user_7|
```

**==> Show the logs where action = clicked**  
Hint : `clickstreamDF.filter(clickstreamDF("action") == "clicked")`  won't work ! :-)  
Let's figure this out.  What is the type of `clickstreamDF("action")`

```
    val c = clickstreamDF("action")
    // c: org.apache.spark.sql.Column = action
    
    // to see methods on c use double-TAB
    c.[TAB][TAB]
```

You will see methods avilable on on `org.apache.spark.sql.Column`

```console

    scala> c.
    !==            %              &&             *              +              -
    /              ===            >              >=             and            as
    asInstanceOf   asc            cast           contains       desc           divide
    endsWith       eqNullSafe     equalTo        explain        geq            getField
    getItem        gt             in             isInstanceOf   isNotNull      isNull
    leq            like           lt             minus          mod            multiply
    notEqual       or             plus           rlike          startsWith     substr
    toString       unary_!        unary_-        ||
```

Now use the correct method to filter 'action = clicked'  
Hint: `===`  or `equalTo`

```scala

    clickstreamDF.filter(clickstreamDF("action") === ??? ).show
    or 
    clickstreamDF.filter(clickstreamDF("action").equalTo(???)).show
```


**==> Count the number of visits from each domain**    
Hint : `val g = clickstreamDF.groupBy("column_name")`  
inspect methods on `g`  (use tab completion)  
then do `count` on `g`  
then do a show  
So `clickstreamDF.groupby().count.show`  


----------------------------
STEP 5 : Joining Dataframes
----------------------------

Let's load another data set `domain info`  
The data is in   `data/click-stream/domain-info.json`  
The data looks like this:

```console

    {"domain":"amazon.com","category":"SHOPPING"}
    {"domain":"bbc.co.uk","category":"NEWS"}
    {"domain":"facebook.com","category":"SOCIAL"}
    ...
```

**==> Load the dataframe**

```scala

    val domainsDF = sqlContext.read.json("data/click-stream/domain-info.json")
```

**==> Join both dataframes**

```scala

    val joined = clickstreamDF.join(domainsDF,  clickstreamDF("domain") === domainsDF("domain"))
    
    // see the results
    joined.show
```

**==> Inspect the results, here is a sample**

```console

    +-------+-----------+----+-----------------+----+----------+-------------+------+-----------+-----------------+
    | action|   campaign|cost|           domain|  ip|   session|    timestamp|  user|   category|           domain|
    +-------+-----------+----+-----------------+----+----------+-------------+------+-----------+-----------------+
    |clicked|campaign_19| 118|      youtube.com|ip_4|session_36|1420070400000|user_9|      VIDEO|      youtube.com|
    |blocked| campaign_2|   7|      youtube.com|ip_2|session_93|1420070412960|user_1|      VIDEO|      youtube.com|
    |blocked|campaign_17|  20|       amazon.com|ip_4|session_13|1420070406048|user_1|   SHOPPING|       amazon.com|
```

**==> Note some rows are missing.  Which ones?  Why?**

**==> Do an outer join**    
Hint : provide a third argument `outer` to the join statement  
e.g  `val joinedOuter = clickstreamDF.join(domainsDF,  ......,    "outer")`

**==> Inspect the output, might look like this**  
**==> Can you explain the null values?**  

```console

    // joinedOuter.show
    
    // result ==> 
    
    +-------+-----------+----+-----------------+----+----------+-------------+------+-----------+-----------------+
    | action|   campaign|cost|           domain|  ip|   session|    timestamp|  user|   category|           domain|
    +-------+-----------+----+-----------------+----+----------+-------------+------+-----------+-----------------+
    |blocked| campaign_9| 139|          cnn.com|ip_8|session_13|1420070404320|user_7|       null|             null|
    |   null|       null|null|             null|null|      null|         null|  null|     SOCIAL|      twitter.com|
    |clicked| campaign_6|  15|comedycentral.com|ip_9|session_49|1420070403456|user_4|       null|             null|

```
