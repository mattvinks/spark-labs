<link rel='stylesheet' href='../assets/main.css'/>

[<< back to main index](../README.md) 

Lab 4.2 MapReduce Application
=============================

### Overview
Write and submit a MapReduce application 

### Depends On 
[4.1-submit.md](4.1-submit.md)

### Run time
20-30 mins

### Setup

During 'mapreduce' lab we did clickstream analysis.  In this lab, we are going to write a full fledged MapReduce program and submit it to Spark. 

## Goal Find click-view ratio for each domain
We want to generate output like this.  The output will be sorted by highest view/click ratio

    # domain, number of total clicks,  number of total views,  views/cliks ratio
    
    npr.org, 10 , 90,  9
    facebook.com,  5, 30, 6


### STEP 1: Inspect Data

Clickstream data has the following format

    timestmap, ip_address, user_id,  action,  domain,  campaign_id,  cost, session
        
    1420070400000,ip_1,user_5,clicked,facebook.com,campaign_6,139,session_98
    1420070400864,ip_2,user_3,viewed,facebook.com,campaign_4,35,session_98
    1420070401728,ip_8,user_8,clicked,youtube.com,campaign_12,115,session_92
    1420070402592,ip_1,user_2,blocked,wikipedia.org,campaign_5,129,session_91

Sample file located at  `~/spark-labs/data/click-stream/clickstream.csv`



### STEP 2: Edit source file

Go to the project root directory

    $    cd   ~/spark-labs/4-api

**=> Edit file : `src/main/scala/x/Clicks.scala`**  
**=> And fix the TODO items**

    $    vi  src/main/scala/x/Clicks.scala
    # or 
    $    nano  src/main/scala/x/Clicks.scala


### STEP 2: Compile the project

**=> Kick off a build**  
(This will take a few minutes for the first time you run it)

    $   sbt package
    # to do a clean rebuild use
    #  sbt clean package


Make sure there are no errors and there is output in `target` dir.

    $  ls -l   target/scala-2.10

You should see output like follows

    drwxr-xr-x  3 sujee  staff   102B Apr 16 09:59 classes/
    -rw-r--r--  1 sujee  staff    13K Apr 16 09:59 testapp_2.10-1.0.jar

`testapp_2.10-1.0.jar `  is our code compiled.
 
### STEP 3: Start Spark Server

    $  ~/spark/sbin/start-all.sh

**=> Check the Spark Server UI at port 8080.**  
**=> Note the Spark master URL.**  

<img src="../images/5.1b.png" style="border: 5px solid grey; max-width:100%;"/>


### STEP 4: Submit the application

Use the following command to submit the job

    $   ~/spark/bin/spark-submit --class 'x.Clicks' --master MASTER_URL  --driver-class-path  logging    target/scala-2.10/testapp_2.10-1.0.jar    <files to process>

* MASTER URL : substitute your spark master url
* for files you can try `data/click-stream/clickstream.csv`

Here is an example

    $   ~/spark/bin/spark-submit --class 'x.Clicks' --master spark://loaclhost:7077   --driver-class-path  logging   target/scala-2.10/testapp_2.10-1.0.jar    '../data/click-stream/clickstream.csv'


**=> Watch the console output**

It may look like this

    ### total clickstream records 20
    ### domain count :
    Map(amazon.com -> 2, funnyordie.com -> 2, nytimes.com -> 1, cnn.com -> 2, youtube.com -> 1, wikipedia.org -> 2, facebook.com -> 2, bbc.co.uk -> 1, npr.org -> 2, foxnews.com -> 3, hulu.com -> 2)
    ### top domains :
    List((foxnews.com,3), (hulu.com,2), (npr.org,2), (facebook.com,2), (wikipedia.org,2), (cnn.com,2), (funnyordie.com,2), (amazon.com,2), (bbc.co.uk,1), (youtube.com,1), (nytimes.com,1))

The lines starting with `###` are output from our program


### STEP 5 : Generate some clickstream data

We have been testing with a small sample file of `data/click-stream/clickstream.csv` file.  Now we are going to generate more data using a data-gen script located in `data/click-stream/gen-clickstream.py`

    $  cd   ~/spark-labs/data/click-stream  #  cd to clickstream data dir
    $  python gen-clickstream.py

This script will generate some files with random clickstream data

    generating log  2015-01-01.csv
    generating log  2015-01-02.csv
    generating log  2015-01-03.csv
    generating log  2015-01-04.csv
    generating log  2015-01-05.csv
    generating log  2015-01-06.csv
    generating log  2015-01-07.csv
    generating log  2015-01-08.csv
    generating log  2015-01-09.csv
    generating log  2015-01-10.csv


### STEP 6 : Process Generated Clickstream Data

    $    cd   ~/spark-labs/5-api
    $    ~/spark/bin/spark-submit --class 'x.Clicks' --master spark://loaclhost:7077   --driver-class-path  logging   target/scala-2.10/testapp_2.10-1.0.jar    '../data/click-stream/*.csv'

Note:
* we are including all the log files using a wild card `*.csv`
* don't forget the single quotes '' 

**=> Note the time it took to process the entire logs.  Compare it with the time to took process a single file**
