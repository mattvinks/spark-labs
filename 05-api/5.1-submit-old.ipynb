{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel='stylesheet' href='../assets/css/main.css'/>\n",
    "\n",
    "[<< back to main index](../README.md)\n",
    "\n",
    "Lab 5.1 First Spark Job Submission\n",
    "==================================\n",
    "\n",
    "### Overview\n",
    "Submit a job for Spark\n",
    "\n",
    "### Depends On \n",
    "None\n",
    "\n",
    "### Run time\n",
    "20-30 mins\n",
    "\n",
    "\n",
    "## Step 0 : Editing Files on VM\n",
    "Please follow [these instructions](../edit-files.md)\n",
    "\n",
    "## STEP 1: Edit source file\n",
    "\n",
    "Go to the project root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "    $    cd ~/spark-labs/05-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Edit file : `~/spark-labs/05-api/python/processfiles.py`**  \n",
    "**=> And fix the TODO items**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## STEP 2: Test Application in Local Master Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "    $  cd  ~/spark-labs/05-api\n",
    "    $ export PYSPARK_PYTHON=python3\n",
    "\n",
    "    $   ~/spark/bin/spark-submit  --master local[*]  python/processfiles.py    README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==> Checkout the Shell UI (4040)**   \n",
    "\n",
    "**==> Hit Enter key to terminate the program**\n",
    "\n",
    "**==> Turn off the logs by sending logs by `2> logs` **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "    $   ~/spark/bin/spark-submit  --master local[*]  python/processfiles.py    README.md  2> logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==> Try a few input files **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "    $   ~/spark/bin/spark-submit  --master local[*]  python/processfiles.py   ~/data/text/twinkle/*  2> logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's submit the application to Spark server\n",
    "\n",
    "## STEP 3: Start Spark Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "    $  ~/spark/sbin/start-all.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Check the Spark Server UI at port 8080.**  \n",
    "**=> Note the Spark master URL.**  \n",
    "\n",
    "<img src=\"../assets/images/4.1b.png\" style=\"border: 5px solid grey; max-width:100%;\"/>\n",
    "\n",
    "\n",
    "## STEP 4: Submit the application\n",
    "\n",
    "Use the following command to submit the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "    $  cd  ~/spark-labs/05-api\n",
    "\n",
    "    # single README file\n",
    "    $   ~/spark/bin/spark-submit  --master MASTER_URL   python/processfiles.py   README.md   2> logs\n",
    "\n",
    "    # multiple twinkle files\n",
    "    $   ~/spark/bin/spark-submit  --master MASTER_URL  python/processfiles.py    ~/data/text/twinkle/*  2> logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MASTER URL : substitute your spark master url\n",
    "* for files you can try `README.md`\n",
    "\n",
    "**=> Watch the console output**\n",
    "\n",
    "It may look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "console"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    15/04/15 15:41:51 INFO SparkUI: Started SparkUI at http://192.168.1.115:4040\n",
    "    ...\n",
    "    15/04/15 15:41:54 INFO DAGScheduler: Job 0 finished: count at ProcessFiles.scala:42, took 2.156893 s\n",
    "\n",
    "    ### README.md : count (7) took 2,251.007000 ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines starting with `###` are output from our program\n",
    "\n",
    "\n",
    "## STEP 5:  Configuring logging\n",
    "\n",
    "#### To quickly turn off the logging:\n",
    "Redirect the logs as follows `  2> logs`.   \n",
    "All logs will be sent to `logs` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "    $  ~/spark/bin/spark-submit  --master MASTER_URL  python/processfiles.py    <files to process>    2>  logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using log4j config\n",
    "Spark by default logs at INFO level.  While there is a lot of useful information there, it can be quite verbose.\n",
    "\n",
    "We have a `logging/log4j.properties` file.  Inspect this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "    $    cat   logging/log4j.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file has following contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set everything to be logged to the console\n",
    "log4j.rootCategory=WARN, console\n",
    "log4j.appender.console=org.apache.log4j.ConsoleAppender\n",
    "log4j.appender.console.target=System.err\n",
    "log4j.appender.console.layout=org.apache.log4j.PatternLayout\n",
    "log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n",
    "\n",
    "# Settings to quiet third party logs that are too verbose\n",
    "log4j.logger.org.eclipse.jetty=WARN\n",
    "log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\n",
    "log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide `--driver-class-path logging/`  to spark-submit to turn off logging\n",
    "\n",
    "Here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "    $   ~/spark/bin/spark-submit --master local[*]  --driver-class-path logging/ processfiles/processfiles.py   README.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
