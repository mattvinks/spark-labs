{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel='stylesheet' href='../assets/css/main.css'/>\n",
    "\n",
    "[<< back to main index](../README.md)\n",
    "\n",
    "Lab 3.1 : RDD Basics operations\n",
    "================================\n",
    "\n",
    "- [Standalone version](3.1-rdd-basics.md) \n",
    "\n",
    "### Overview\n",
    "* Learning basic operations like filter / map / count\n",
    "* work with larger sized RDDs\n",
    "* Load multiple files into a single RDD\n",
    "* Save computed RDDs\n",
    "\n",
    "\n",
    "\n",
    "### Depends On\n",
    "None\n",
    "\n",
    "### Run time\n",
    "30-40 mins\n",
    "\n",
    "\n",
    "## STEP 1:  Do work in the notebook, or fire up pyspark shell\n",
    "\n",
    "You can perform many of the steps right here in the notebook, if you started jupyter properly within pyspark.  Otherwise, open up a pyspark shell.  That can be done by executing \n",
    "\n",
    "```\n",
    "   ~/spark/bin/pyspark\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Load a simple text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = sc.textFile(\"/data/text/twinkle/sample.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> what is the 'type' of f ?**  \n",
    "Hint : just type `f` in the shell\n",
    "Here is a possible output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Filter\n",
    "Let's find how many lines contain the word 'twinkle'\n",
    "We will use the 'filter' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered = f.filter(lambda line: \"twinkle\" in line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After entering the above in pyspark...  \n",
    "**=> Goto Spark shell UI **  \n",
    "**=> Inspect the 'Stages' section in the UI.**  \n",
    "**=> How is the filter executed? Can you explain the behavior?**  \n",
    "\n",
    "**=> Count how many lines contain the word 'twinkle'**  \n",
    "hint : apply `count()` to `filtered` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply count() to variable here.\n",
    "print(f.count())\n",
    "print(filtered.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "console"
     ],
     "id": ""
    }
   },
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Check the Stages in UI,  what do you see?**  \n",
    "**=> How long did the job take?**  \n",
    "**=> Print out all the lines containing the word 'twinkle'**   \n",
    "Hint : `collect()`  \n",
    "Here is a sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Print out all the lines containing twinkle here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "console"
     ],
     "id": ""
    }
   },
   "source": [
    "```\n",
    "[u'twinkle twinkle little star', u'twinkle twinkle little star']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**=> Checkout 'DAG' visualization**\n",
    "\n",
    "<img src=\"../assets/images/3.1c.png\" style=\"border: 5px solid grey; max-width:100%;\"/>\n",
    "\n",
    "**=> Quit pyspark using 'exit'  or pressing  Control+D**\n",
    "\n",
    "\n",
    "## STEP 4:  Large data sets\n",
    "**==> Quit previous pyspark session, if you haven't done so yet.. `Control + D`**  \n",
    "\n",
    "We have some large data sets of 'twinkle' data generated in `~/data/text/twinkle`  directory.\n",
    "\n",
    "**=> Verify the data files and their sizes by doing a**\n",
    "\n",
    "``bash\n",
    "    $   l ~/data/text/twinkle\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output might look like this\n",
    "\n",
    "<img src=\"../assets/images/3.1a.png\" style=\"border: 5px solid grey; max-width:100%;\"/>\n",
    "\n",
    "We are going to use these files in spark\n",
    "\n",
    "**=> [Optional] You can use the following script to generate more data if you need to**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$   cd  /data/text/twinkle\n",
    "$   ./create-data-files.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will generate a bunch of data files at various sizes (1M, 10M, 100M, 500M and 1G)\n",
    "\n",
    "## STEP 5:  Start shell With More Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder of this lab should be done in the pyspark shell rather than in the notebook.  We need more memory than what the notebook can provide.\n",
    "\n",
    "```bash\n",
    "$   ~/spark/bin/pyspark  --executor-memory 1G  --driver-memory 1G\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: Process a large file\n",
    "**=> In PySpark Shell, load `data/text/twinkle/100M.data`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = sc.textFile(\"/data/text/twinkle/100M.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Count number of lines that have the word \"diamond\"**  \n",
    "hint : `filter`  and `count`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**=> How many 'tasks' are used in the above calculation**  \n",
    "Hint : Check spark shell UI\n",
    "\n",
    "<img src=\"../assets/images/3.1b.png\" style=\"border: 5px solid grey; max-width:100%;\" />\n",
    "\n",
    "**=> Can you explain the number of tasks?**  \n",
    "Hint : check number of partitions in RDD using `getNumPartitions`  or `partitions.length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Count number of lines that does NOT have the word 'diamond'**  \n",
    "Hint : use negative operator  !  \n",
    "`f.filter(lambda line: \"diamond\" not in line) `\n",
    "\n",
    "**=> Verify both counts add up to the total line count**\n",
    "\n",
    "**=> Notice the time taken for each operation**\n",
    "\n",
    "**=> Try the above with larger data files : 500M.data  ... 1G.data**\n",
    "  - note the times taken\n",
    "  - how many tasks?\n",
    "\n",
    "## STEP 7: Loading multiple files\n",
    "**=> Load all *.data files under  /data/text/twinkle  directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    f = spark.read.textFile(\"/data/text/twinkle/*.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Do a count() on RDD.**  \n",
    "Notice the partition count and time taken to execute\n",
    "Verify partition count from Spark-Shell UI\n",
    "\n",
    "## STEP 8:  Saving the RDD\n",
    "Continuing with the big RDD created on step (5)....\n",
    "\n",
    "**=> Create a new RDD by filtering first RDD for word 'diamond'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered = f.filter(lambda line: \"diamond\" not in line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Save the new RDD into a directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered.saveAsTextFile(\"out1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Inspect the console output during the run.**\n",
    "\n",
    "Inspect the output directory\n",
    "You can use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$    ls -lh   out1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see files created use :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$   less  out1/part-00000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> What do you see as output?**\n",
    "\n",
    "\n",
    "## Bonus Lab: Merging partitions into a single one\n",
    "When we saved data in the above section, there are multiple files created in output directory.   Can you just create one output file?   \n",
    "Hint : see the API for `coalesce or repartition`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
