{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1 : RDD Basics operations\n",
    "\n",
    "\n",
    "### Overview\n",
    "* Learning basic operations like filter / map / count\n",
    "* work with larger sized RDDs\n",
    "* Load multiple files into a single RDD\n",
    "* Save computed RDDs\n",
    "\n",
    "### Depends On\n",
    "None\n",
    "\n",
    "### Run time\n",
    "30-40 mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Spark...\n",
      "Spark found in :  /home/ubuntu/spark\n",
      "Spark config:\n",
      "\t spark.app.name=TestApp\n",
      "\tspark.master=local[*]\n",
      "\texecutor.memory=2g\n",
      "\tspark.sql.warehouse.dir=/tmp/tmpjpr5vfor\n",
      "\tsome_property=some_value\n",
      "Spark UI running on port 4044\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-16-0-107.ec2.internal:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TestApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8a5c47d438>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize Spark Session\n",
    "import os\n",
    "import sys\n",
    "top_dir = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "if top_dir not in sys.path:\n",
    "    sys.path.append(top_dir)\n",
    "\n",
    "from init_spark import init_spark\n",
    "spark = init_spark()\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Load a simple text file\n",
    "This uses the classic RDD API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/text/twinkle/sample.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "f = sc.textFile(\"/data/text/twinkle/sample.txt\")\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> what is the 'type' of f ?**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Filter\n",
    "Let's find how many lines contain the word 'twinkle'\n",
    "We will use the 'filter' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[2] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "filtered = f.filter(lambda line: \"twinkle\" in line)\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After entering the above in pyspark...  \n",
    "\n",
    "**=> Goto Spark shell UI **  \n",
    "**=> Inspect the 'Stages' section in the UI.**  \n",
    "**=> How is the filter executed? Can you explain the behavior?**  \n",
    "**=> Count how many lines contain the word 'twinkle'**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# TODO: Apply count() to 'filtered' here.\n",
    "print(filtered.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Check the Stages in UI,  what do you see?**  \n",
    "**=> How long did the job take?**  \n",
    "**=> Print out all the lines containing the word 'twinkle'**   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['twinkle twinkle little star', 'twinkle twinkle little star']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print out all the lines containing twinkle here:\n",
    "# Hint : collect()\n",
    "print(filtered.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample output\n",
    "```\n",
    "[u'twinkle twinkle little star', u'twinkle twinkle little star']\n",
    "```\n",
    "\n",
    "**=> Checkout 'DAG' visualization**\n",
    "\n",
    "<img src=\"../assets/images/3.1c.png\" style=\"border: 5px solid grey; max-width:100%;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "console"
     ],
     "id": ""
    }
   },
   "source": [
    "## STEP 3:  Large data sets\n",
    "\n",
    "We have some large data sets of 'twinkle' data generated in `/data/text/twinkle`  directory.\n",
    "\n",
    "<img src=\"../assets/images/3.1a.png\" style=\"border: 5px solid grey; max-width:100%;\"/>\n",
    "\n",
    "We are going to use these files in spark\n",
    "\n",
    "#### [Optional] You can use the following script to generate more data if you need to\n",
    "```bash\n",
    "$   cd  /data/text/twinkle\n",
    "$   ./create-data-files.sh\n",
    "```\n",
    "This script will generate a bunch of data files at various sizes (1M, 10M, 100M, 500M and 1G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: Process a large file\n",
    "**=> Load `/data/text/twinkle/100M.data`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/text/twinkle/100M.data MapPartitionsRDD[5] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "f = sc.textFile(\"/data/text/twinkle/100M.data\")\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Count number of lines that have the word \"diamond\"**  \n",
    "hint : `filter`  and `count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782519\n"
     ]
    }
   ],
   "source": [
    "## TODO : keyword  = 'diamond'\n",
    "filtered = f.filter(lambda line: \"diamond\" in line)\n",
    "\n",
    "## hint : count\n",
    "print(filtered.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**=> How many 'tasks' are used in the above calculation**  \n",
    "Hint : Check spark shell UI\n",
    "\n",
    "<img src=\"../assets/images/3.1b.png\" style=\"border: 5px solid grey; max-width:100%;\" />\n",
    "\n",
    "**=> Can you explain the number of tasks?**  \n",
    "Hint : check number of partitions in RDD using `getNumPartitions`  or `partitions.length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5:  Saving the RDD\n",
    "\n",
    "**=> Create a new RDD by filtering first RDD for word 'diamond'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "f = sc.textFile(\"/data/text/twinkle/100M.data\")\n",
    "filtered = f.filter(lambda line: \"diamond\" not in line)\n",
    "filtered.coalesce(1).saveAsTextFile(\"outro\")\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Inspect the output directory**\n",
    "\n",
    "In Jupyter, go to **03-rdd/out** directory.\n",
    "\n",
    "**=> What do you see as output?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Lab: Merging partitions into a single one\n",
    "When we saved data in the above section, there are multiple files created in output directory.   Can you just create one output file?   \n",
    "Hint : see the API for `coalesce or repartition`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
