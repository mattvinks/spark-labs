{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel='stylesheet' href='../assets/css/main.css'/>\n",
    "\n",
    "[<< back to main index](../README.md)\n",
    "\n",
    "Lab 3.6 : RDD Caching\n",
    "=====================\n",
    "\n",
    "### Overview\n",
    "Understanding RDD caching\n",
    "\n",
    "### Depends On \n",
    "None\n",
    "\n",
    "### Run time\n",
    "15-20 mins\n",
    "\n",
    "\n",
    "### STEP 1: Inspect data\n",
    "\n",
    "Under `~/data/twinkle` directory we have created some large data files for you.  Verify this by doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "```bash\n",
    "    $    l   ~/data/twinkle\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see files like  '100M.data', '500M.data' ... so on.\n",
    "Your output might look like this  \n",
    "<img src=\"../images/3.1a.png\" style=\"border: 5px solid grey; max-width:100%;\"/>\n",
    "\n",
    "#### Optional Step\n",
    "You can generate more data if you'd like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%bash\n",
    "    $    cd ~/spark-labs/data/twinkle\n",
    "    $    ./create-data-files.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will generate a bunch of data files at various sizes (1M, 10M, 100M, 500M and 1G)\n",
    "Verify the data files and their sizes by doing a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$   ls -lh\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Start Spark Server / Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$   cd   # home directory\n",
    "$  ~/spark/sbin/start-all.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting Shell (with 4G memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%bash\n",
    "    $  ~/spark/bin/spark-shell   --executor-memory 4G  --master <spark master uri> \n",
    "    #                                                           ^^^^^^^^^^^^^^^^\n",
    "    #                                                            update this to match your spark server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also set the logging level to INFO (so Spark will print out job execution times on console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Recording Caching times\n",
    "Download and inspect the Excel worksheet : [caching-worksheet](caching-worksheet.xlsx).   \n",
    "We are going to fill in the values here to understand how caching performs.\n",
    "\n",
    "It looks like this:\n",
    "<img src=\"../images/3.6a.png\" style=\"border: 5px solid grey; max-width:100%;\"/>\n",
    "\n",
    "\n",
    "### STEP 4: Load RDD\n",
    "\n",
    "Load a big file (e.g 500M.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = sc.textFile(\"data/twinkle/500M.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Count the number of lines in this file**    \n",
    "Hint : `count()`  \n",
    "\n",
    "**=> Notice the time took**   \n",
    "Look for output like this in Spark Shell console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "console"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Job 1 finished: count at <console>:30, took __3.792822__ s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Do the same count() operation a few times until the execution time 'stablizes'**  \n",
    "**=> Record the time in spreadsheet.**  \n",
    "**=> Can you explain the behavior of count() execution time ?**\n",
    "\n",
    "\n",
    "### STEP 5:  Cache\n",
    "\n",
    "**=> Cache the file using  `cache()` action.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Run the `count()` again. Notice the time.   Can you explain this behavior ?  :-)** \n",
    "\n",
    "**=> Run count() a few more times and note the execution times.**  \n",
    "**=> Record the time in spreadsheet.**  \n",
    "**=> Do the timings make sense?** \n",
    "\n",
    "\n",
    "### STEP 6:  Understanding Cache storage\n",
    "\n",
    "Go to spark shell UI @ port 4040  \n",
    "**=> Inspect 'storage' tab**  \n",
    "\n",
    "<img src=\"../images/3.6b.png\" style=\"border: 5px solid grey; max-width:100%;\"/>\n",
    "\n",
    "**=> Can you see the cached RDD?  What is the memory size?**  \n",
    "**=> What are the implications?** \n",
    "\n",
    "### STEP 7:  Cache a larger file\n",
    "\n",
    "**=> Try to cache 1G.data file and do count()**  \n",
    "Is caching successful ?\n",
    "If not, try starting Spark shell with more memory\n",
    "\n",
    "\n",
    "### Step 8 : Caching RDD vs. Dataframe\n",
    "We will load the same data using RDD API and Dataframe API will compare cache performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RDD\n",
    "rdd = sc.textFile(\"data/twinkle/100M.data\")\n",
    "rdd.cache()\n",
    "rdd.count()  # force caching\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the 'Storage' tab in Spark Shell UI (port 4040).  \n",
    "\n",
    "Here is a sample output.\n",
    "\n",
    "<img src=\"../images/3.6c-rdd-ds-cache.png\" style=\"border: 5px solid grey; max-width:100%;\"/>\n",
    "\n",
    "** ==> Discuss your findings **\n",
    "\n",
    "\n",
    "\n",
    "### Step 9 : Reducing memory footprint \n",
    "\n",
    "There are various levels of memory caching.  Here are a couple:  \n",
    "\n",
    "* Raw caching (`rdd.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY)`)  \n",
    "* Serialized Caching (`rdd.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY_SER)`)\n",
    "\n",
    "\n",
    "**=> Try both options `f.persist(....)` .  Monitor memory consumption in storage tab**\n",
    "\n",
    "**NOte: Caching level can not be changed after an RDD cached.  You have to 'uncache / unpersist' the RDD and then cache it again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd.persist(pyspark.StorageLevel.MEMORY_ONLY) # same as rdd.cache()\n",
    "rdd.unpersist()\n",
    "rdd.persist(pyspark.StorageLevel.MEMORY_ONLY_SER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group discussion\n",
    "\n",
    "* mechanics of caching\n",
    "* implications of caching vs memory\n",
    "\n",
    "\n",
    "\n",
    "### BONUS LAB : Caching data from Amazon S3\n",
    "\n",
    "We have some data files stored in Amazon S3.  Here are couple of path names\n",
    "* s3n://elephantscale-public/data/twinkle/100M.data\n",
    "* s3n://elephantscale-public/data/twinkle/200M.data\n",
    "* s3n://elephantscale-public/data/twinkle/500M.data\n",
    "* and more...\n",
    "\n",
    "If you have trouble with s3n protocol, you can first download the files with URL like\n",
    "\n",
    "* https://s3.amazonaws.com/elephantscale-public/data/twinkle/100M.data\n",
    "* https://s3.amazonaws.com/elephantscale-public/data/twinkle/200M.data\n",
    "\n",
    "**=> Try loading these files.  Measure performance time before and after caching.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = sc.textFile(\"s3n://elephantscale-public/data/twinkle/200M.data\")\n",
    "f.count() // measure time.. do it a couple of times\n",
    "f.cache() \n",
    "f.count() // measure time.. do it a couple of times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "* [Understanding Spark Caching by Sujee Maniyam](http://sujee.net/2015/01/22/understanding-spark-caching/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
