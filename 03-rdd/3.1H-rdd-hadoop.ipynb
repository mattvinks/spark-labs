{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel='stylesheet' href='../assets/css/main.css'/>\n",
    "\n",
    "# 3.1 H: Loading Data (RDD) from HDFS\n",
    "\n",
    "- [Standalone version](3.1-rdd-basics.md) \n",
    "- [Hadoop version](3.1H-rdd-hadoop.md)\n",
    "\n",
    "### Overview\n",
    "* Learning basic operations like filter / map / count\n",
    "* work with larger sized RDDs\n",
    "* Load multiple files into a single RDD\n",
    "* Save computed RDDs\n",
    "\n",
    "### Depends On\n",
    "None\n",
    "\n",
    "### Run time\n",
    "30-40 mins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Load a simple text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = sc.textFile(\"/data/twinkle/sample.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> what is the 'type' of f ?**  \n",
    "Hint : just type `f` in the shell\n",
    "Here is a possible output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Filter\n",
    "Let's find how many lines contain the word 'twinkle'\n",
    "We will use the 'filter' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered = f.filter(lambda line: \"twinkle\" in line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After entering the above in Spark-shell...  \n",
    "**=> Goto Spark shell UI **  \n",
    "**=> Inspect the 'Stages' section in the UI.**  \n",
    "**=> How is the filter executed? Can you explain the behavior?**  \n",
    "\n",
    "**=> Count how many lines contain the word 'twinkle'**  \n",
    "hint : apply `count()` to `filtered` variable\n",
    "\n",
    "Here is a sample output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "attributes": {
     "classes": [
      "console"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "source": [
    "```\n",
    "15/03/31 23:19:30 INFO DAGScheduler: Stage 0 (count at <console>:17) finished in 0.074 s\n",
    "15/03/31 23:19:30 INFO DAGScheduler: Job 0 finished: count at <console>:17, took 0.141676 s\n",
    "2  <--- this is the result of count()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Check the Stages in UI,  what do you see?**  \n",
    "**=> How long did the job take?**  \n",
    "**=> Print out all the lines containing the word 'twinkle'**   \n",
    "Hint : `collect()`  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "console"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Print out all the lines containing the word 'twinkle'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Here is a sample output\n",
    "```\n",
    "twinkle twinkle little star, twinkle twinkle little star\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Checkout 'DAG' visualization**\n",
    "\n",
    "<img src=\"../assets/images/3.1c.png\" style=\"border: 5px solid grey; max-width:100%;\"/>\n",
    "\n",
    "**=> Quit Spark-shell using 'exit'  or pressing  Control+D**\n",
    "\n",
    "\n",
    "## STEP 4:  Large data sets\n",
    "**==> Quit previous spark-shell session, if you haven't done so yet.. `Control + D`**  \n",
    "\n",
    "We have some large data sets of 'twinkle' data generated in `/data/twinkle`  directory.\n",
    "\n",
    "<img src=\"../assets/images/3.1a.png\" style=\"border: 5px solid grey; max-width:100%;\"/>\n",
    "\n",
    "## STEP 5:  Start shell With More Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$   pyspark  --executor-memory 1G  --driver-memory 1G\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: Process a large file\n",
    "**=> In PySpark Shell, load `data/twinkle/100M.data`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = sc.textFile(\"/data/twinkle/100M.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Count number of lines that have the word \"diamond\"**  \n",
    "hint : `filter`  and `count`\n",
    "\n",
    "**=> How many 'tasks' are used in the above calculation**  \n",
    "Hint : Check spark shell UI\n",
    "\n",
    "<img src=\"../assets/images/3.1b.png\" style=\"border: 5px solid grey; max-width:100%;\" />\n",
    "\n",
    "**=> Can you explain the number of tasks?**  \n",
    "Hint : check number of partitions in RDD using `getNumPartitions`  or `partitions.length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Count number of lines that does NOT have the word 'diamond'**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Verify both counts add up to the total line count**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**=> Verify both counts add up to the total line count**\n",
    "\n",
    "**=> Notice the time taken for each operation**\n",
    "\n",
    "**=> Try the above with larger data files : 500M.data  ... 1G.data**\n",
    "  - note the times taken\n",
    "  - how many tasks?\n",
    "\n",
    "## STEP 7: Loading multiple files\n",
    "**=> Load all *.data files under  data/twinkle  directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = sc.textFile(\"/data/twinkle/*.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Do a count() on RDD.**  \n",
    "Notice the partition count and time taken to execute\n",
    "Verify partition count from Spark-Shell UI\n",
    "\n",
    "## STEP 8:  Saving the RDD\n",
    "Continuing with the big RDD created on step (5)....\n",
    "\n",
    "**=> Create a new RDD by filtering first RDD for word 'diamond'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered = f.filter(lambda line: \"diamond\" in line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Save the new RDD into a directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered.saveAsTextFile(\"MY_NAME_out\")  # fix MY_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Inspect the output directory using HDFS File browser **\n",
    "\n",
    "**=> What do you see as output?**\n",
    "\n",
    "\n",
    "## Bonus Lab: Merging partitions into a single one\n",
    "When we saved data in the above section, there are multiple files created in output directory.   Can you just create one output file?   \n",
    "Hint : see the API for `coalesce or repartition`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
